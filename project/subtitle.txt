One of the most exciting developments
in deep learning has been the transformer Network, or
sometimes called Transformers. This is an architecture that has
completely taken the NLP world by storm. And many of the most effective algorithms for NLP today are based on
the transformer architecture. It is a relatively complex neural
network architecture, but in this and the next three videos after this
will go through it piece by piece. So that by the end of this next four
videos, you have a good sense of how the transformer Network works and
you'll be able to apply to your problems. As the complexity of your
sequence task increases, so does the complexity of your model. We have started this
course with the RNN and found that it had some problems
with vanishing gradients, which made it hard to capture long
range dependencies and sequences. We then looked at the GRU and then
the LSTM model as a way to resolve many of those problems where you make use of
gates to control the flow of information. And so each of these units
had a few more computations. While these editions improved control
over the flow of information, they also came with increased complexity. So as we move from our RNNs to GRU to
LSTM ,the models became more complex. And all of these models
are still sequential models in that they ingested the input, maybe the input sentence one word or
one token at the time. And so, as as if each unit was like
a bottleneck to the flow of information. Because to compute the output
of this final unit, for example, you first have to compute the outputs
of all of the units that come before. In this video, you learned about the
transformer architecture, which allows you to run a lot more of these computations
for an entire sequence in parallel. So you can ingest an entire
sentence all at the same time, rather than just processing it one
word at a time from left to right. The Transformer Network was published
in a seminal paper by Ashish Vaswani , Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser and Illia Polosukhin. One of the inventors of
the Transformer network, Lukasz Kaiser, is also co instructor of the NLP
specialization with deep learning dot AI. So you can check that out as well when
you're done with this deep learning specialization. The major innovation of
the transformer architecture is combining the use of attention
based representations and a CNN convolutional neural
network style of processing. So an RNN may process
one output at the time, and so maybe y(0) feeds in to
them that you compute y(1) and then this is used to compute y(2). This is a very sequential
way of processing tokens, and you might contrast this with a CNN or confident that can take
input a lot of pixels. Yeah, or maybe a lot of words and can compute representations for
them in parallel. So what you see in the Attention Network
is a way of computing very rich, very useful representations of words. But with something more akin to this
CNN style of parallel processing. To understand the attention network, there will be two key ideas will
go through in the next few videos. The first is self attention. The goal of self attention is,
if you have, say, a sentence of five words will end up
computing five representations for these five words,
was going to write A1,A2,A3, A4 and A5. And this will be an attention based
way of computing representations for all the words in your
sentence in parallel. Then multi headed attention is basically for
loop over the self attention process. So you end up with multiple
versions of these representations. And it turns out that
these representations, which will be very rich representations,
can be used for machine translation or
other NLP tasks to great effectiveness. So in the next video, let's jump
in to learn about self attention, to compute these rich representations. The video after that,
we'll talk about multi headed attention. And then the final video on transforming
networks will put all of these together so that you understand how the entire
transformer architecture works into end. Let's go to the next video.
Let's jump in to talk about the self-attention
mechanism of transformers. If you can get the main
idea behind this video, you'll understand the
most important core idea behind what makes
transformer networks work. Let's jump in. You've seen how
attention is used with sequential neural
networks such as RNNs. To use attention with a
style more late CNNs, you need to calculate
self-attention, where you create
attention-based representations for each of the words
in your input sentence. Let's use our running example, Jane, visite, l'Afrique,
en, septembre, our goal will be for each word to compute an attention-based
representation like this. So we'll end up
with five of these, since our sentence
has five words. When we've computed them we'll call the five
representations with these five words A1through A5. I know you're starting
to see a bunch of symbols Q, K, and V, we'll explain what
these symbols mean in a later slide so don't
worry about them for now. The running example
I'm going to use is take the word l'Afrique
in this sentence. We'll step through on
the next slide how the transformer network's
self-attention mechanism allows you to compute
A3 for this word, and then you do
the same thing for the other words in
the sentence as well. Now you learn previously
about word embeddings. One way to represent
l'Afrique would be to just look up the word
embedding for l'Afrique. But depending on the context, are we thinking of l'Afrique
or Africa as a site of historical interests or
as a holiday destination, or as the world's second
largest continent. Depending on how you're
thinking of l'Afrique, you may choose to
represent it differently, and that's what this
representation A(3) will do. It will look at the
surrounding words to try to figure out what's actually going on in how we're talking about
Africa in this sentence, and find the most appropriate
representation for this. In terms of the
actual calculation, it won't be too different from the attention mechanism you saw previously as applied
in the context of RNNs, except we'll compute
these representations in parallel for all five
words in a sentence. When we're building
attention on top of RNNs, this was the equation we used. With the self-attention
mechanism, the attention equation is instead going to look like this. You can see the equations
have some similarity. The inner term here also
involves a softmax, just like this term
over here on the left, and you can think of the exponent terms as being
akin to attention values. Exactly how these
terms are worked out you'll see in the next slide. So again, don't worry about
the details just yet. But the main
difference is that for every word, say for l'Afrique, you have three values called
the query, key, and value. These vectors are
the key inputs to computing the attention
value for each words. Now, let's step through the steps needed to actually calculate A3. On this slide, let's step through the computations you need
to go from the words l'Afrique to the self-attention
representation A3. For reference, I've
also printed up here on the upper-right that softmax-like equation from the previous slide. First, we're going to associate
each of the words with three values called the
query key and value pairs. If X3 is the word
embedding for l'Afrique, the way that's Q3 is computed
is as a learned matrix, which I'm going to
write as WQ times X3, and similarly for the
key and value pairs, so K3 is WK times X3 and V3 is WV times X3. These matrices, WQ, WK, and WV, are parameters of this
learning algorithm, and they allow you to
pull off these query, key, and value vectors
for each word. So what are these query key and value vectors supposed to do? They were named using a loose
analogy to a concept in databases where you can have queries and also key-value pairs. If you're familiar with
those types of databases, the analogy may
make sense to you, but if you're not familiar with that database concept,
don't worry about it. Let me give one intuition behind the intent of these query, key, and value of vectors. Q3 is a question that you
get to ask about l'Afrique. Q3 may represent a question
like, what's happening there? Africa, l'Afrique
is a destination. You might want to know when computing A^3, what's
happening there. What we're going to do is compute the inner product
between q^3 and k^1, between Query 3 and Key 1, and this will tell us
how good is an answer where it's one to the question of what's
happening in Africa. Then we compute the
inner product between q^3 and k^2 and this
is intended to tell us how good is visite an
answer to the question of what's happening in Africa and so on for the other words
in the sequence. The goal of this operation is to pull up the most
information that's needed to help us compute the most useful
representation A^3 up here. Again, just for
intuition building if k^1 represents that
this word is a person, because Jane is a person, and k^2 represents
that the second word, visite, is an action, then you may find that
q^3 inter producted with k^2 has the largest value, and this may be
intuitive example, might suggest that visite, gives you the most
relevant contexts for what's happening in Africa. Which is that, it's viewed as
a destination for a visit. What we will do is take these five values in this row and compute
a Softmax over them. There's actually this
Softmax over here, and in the example that
we've been talking about, q^3 times k^2 corresponding to word visite maybe
has the largest value. I'm going to shade
that blue over here. Then finally, we're going to take these Softmax values and
multiply them with v^1, which is the value for word 1, the value for word 2, and so on, and so these values correspond
to that value up there. Finally, we sum it all up. This summation corresponds to this summation operator
and so adding up all of these values
gives you A^3, which is just equal
to this value here. Another way to write
A^3 is really as A, this A up here of q^3, k, v. But sometimes it will be more convenient to just
write A^3 like that. The key advantage of
this representation is the word of l'Afrique isn't
some fixed word embedding. Instead, it lets the
self-attention mechanism realize that l'Afrique is
the destination of a visite, of a visit, and thus
compute a richer, more useful representation
for this word. Now, I've been using
the third word, l'Afrique as a running example but you could use this
process for all five words in your sequence to get similarly rich
representations for Jane, visite, l'Afrique, en, septembre. If you put all of these
five computations together, denotation used in
literature looks like this, where you can summarize all of these computations
that we just talked about for all the words in the sequence by
writing Attention(Q, K, V) where Q, K, V matrices with all
of these values, and this is just a compressed or
vectorized representation of this equation up here. The term in the denominator is just to scale the dot-product, so it doesn't explode. You don't really need
to worry about it. But another name for
this type of attention is the scaled
dot-product attention. This is the one represented in the original transformer
architecture paper, Attention Is All
You Need As Well. That's the
self-attention mechanism of the transformer network. To recap, associated with each of the five words you
end up with a query, a key, and a value. The query lets you ask a
question about that word, such as what's
happening in Africa. The key looks at all
of the other words, and by the similarity
to the query, helps you figure out
which words gives the most relevant answer
to that question. In this case, visite is what's happening in Africa,
someone's visiting Africa. Then finally, the value
allows the representation to plug in how visite should
be represented within A^3, within the representation
of Africa. This allows you to come up with a representation for the word Africa that says this is Africa and someone
is visiting Africa. This is a much more nuanced, much richer representation
for the world than if you just had to pull up the
same fixed word embedding for every single word without being able
to adapt it based on what words are to the left and to the
right of that word. We've all got to take into
account and in the context. Now, you have learned about
the self-attention mechanism. We're going to put
a big for-loop over this whole thing and that will be the multi-headed
attention mechanism. Let's dive into the details
of that in the next video.