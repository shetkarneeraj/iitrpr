{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_transcript(filename, model='llama3.2'):\n",
    "    task_description = \"\"\"\n",
    "        You are an AI tasked with generating multiple-choice questions (MCQs) from a given transcript. \n",
    "        Your goal is to:\n",
    "        1. Identify important concepts, events, or details in the transcript.\n",
    "        2. Frame questions in a simple and clear manner based on these concepts.\n",
    "        3. Provide 4 answer options for each question, ensuring one is correct and the others are plausible but incorrect.\n",
    "        4. Specify the index (0-based) of the correct answer for each question.\n",
    "        5. Format your response as a JSON list where each entry follows the structure:\n",
    "        { \"question\": \"<question_text>\", \"options\": [\"<option1>\", \"<option2>\", \"<option3>\", \"<option4>\"], \"correct_answer\": <index_of_correct_option> }\n",
    "\n",
    "        Example output:\n",
    "        [\n",
    "            {\n",
    "                \"question\": \"What is the capital of France?\",\n",
    "                \"options\": [\"Berlin\", \"Madrid\", \"Paris\", \"Rome\"],\n",
    "                \"correct_answer\": 2\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Which planet is known as the Red Planet?\",\n",
    "                \"options\": [\"Earth\", \"Mars\", \"Jupiter\", \"Venus\"],\n",
    "                \"correct_answer\": 1\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the chemical symbol for water?\",\n",
    "                \"options\": [\"H2O\", \"O2\", \"CO2\", \"NaCl\"],\n",
    "                \"correct_answer\": 0\n",
    "            }\n",
    "        ]\n",
    "        Your input will be a transcript, and you will generate 3 questions based on its content in this exact format.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        transcript = file.read()\n",
    "\n",
    "    \n",
    "\n",
    "    prompt = task_description + '\\n Here is the transcript content: \\n' + transcript + 'Generate 3 questions as a JSON list, each question following the specified json format { \"question\": \"<question_text>\", \"options\": [\"<option1>\", \"<option2>\", \"<option3>\", \"<option4>\"], \"correct_answer\": <index_of_correct_option> }.'\n",
    "\n",
    "\n",
    "    response = ollama.generate(model=model, prompt=prompt)\n",
    "\n",
    "    print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a key advantage of using GRUs over LSTMs?\",\n",
      "    \"options\": [\"GRUs are more powerful and flexible\", \"GRUs are simpler models that scale better\", \"LSTMs are easier to build bigger networks\", \"GRUs can capture longer range dependencies\"],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is a common variation of LSTMs?\",\n",
      "    \"options\": [\"Peephole connection with only t-1 values\", \"Peephole connection with both t-1 and x_t values\", \"Peephole connection that affects all gates\", \"Peephole connection that only affects the first gate\"],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which algorithm is more commonly used as a default choice, despite LSTMs being more historically proven?\",\n",
      "    \"options\": [\"GRUs are always preferred\", \"LSTMs are often used in new projects\", \"The choice between GRUs and LSTMs depends on the problem\", \"There isn't a universally superior algorithm\"],\n",
      "    \"correct_answer\": 2\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that I've based my questions on the provided text, but feel free to adjust or add more as needed!\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the text in the JSON format:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a key advantage of using a GRU over an LSTM?\",\n",
      "    \"options\": [\"GRUs are more powerful and flexible\", \"GRUs are simpler models\", \"GRUs are faster to compute\", \"GRUs are better for scaling larger networks\"],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the purpose of a peephole connection in LSTMs?\",\n",
      "    \"options\": [\"To make the gates more independent of each other\", \"To reduce the computational complexity of the model\", \"To allow the gate values to depend on both t-1 and x_t as well as c_(t-1)\", \"To eliminate the need for memory cells\"],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why do researchers recommend using LSTMs over GRUs for certain problems?\",\n",
      "    \"options\": [\"Because LSTMs are simpler models that are easier to build and scale\", \"Because LSTMs have been historically more proven and widely used\", \"Because LSTMs can capture longer range dependencies than GRUs\", \"Because LSTMs are faster to compute than GRUs\"],\n",
      "    \"correct_answer\": 2\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note: The correct answers are based on the information provided in the text and may not be universally true.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a peephole connection in LSTMs?\",\n",
      "    \"options\": [\"A type of activation function\", \"A technique to improve memory cell accessibility\", \"A variation of the LSTM model where gate values depend on c t -1\", \"A way to reduce computational complexity\"],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which algorithm is considered more powerful and flexible?\",\n",
      "    \"options\": [\"GRU\", \"LSTM\", \"Both are equally good for all problems\", \"Neither is more powerful than the other\"],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is a benefit of using GRUs over LSTMs?\",\n",
      "    \"options\": [\"They are more powerful and flexible\", \"They are simpler to build and scale\", \"They can only capture shorter-range dependencies\", \"They are not suitable for long-term memory\"],\n",
      "    \"correct_answer\": 1\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that the `correct_answer` index is 0-based, so if you want to use a 1-based index, you would need to subtract 1 from each value.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a peephole connection in LSTMs?\",\n",
      "    \"options\": [\"A type of activation function\", \"A way to reduce memory usage\", \"A connection that allows the gate values to depend on both a t-1 and x t\", \"A technique for improving model convergence\"],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which algorithm is more powerful and flexible than GRUs?\",\n",
      "    \"options\": [\"GRUs only\", \"LSTMs only\", \"Both LSTMs and GRUs are equally powerful\", \"Neither GRUs nor LSTMs are the better choice\"],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why is the LSTM considered more proven than GRU?\",\n",
      "    \"options\": [\"Because it has fewer gates\", \"Because it has a simpler architecture\", \"Because it has been historically more widely used\", \"Because it is only used for specific tasks\"],\n",
      "    \"correct_answer\": 3\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note: The correct answers are based on the information provided in the text and may not be absolute or definitive.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the text in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is the main difference between GRUs and LSTMs?\",\n",
      "    \"options\": [\"GRU has only one gate\", \"LSTM has more gates than GRU\", \"LSTM is a simpler model than GRU\", \"LSTM is faster to compute\"],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why are LSTMs considered more powerful and flexible than GRUs?\",\n",
      "    \"options\": [\"Because of the additional gate that affects all elements of the hidden state\"], \n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is a peephole connection in an LSTM model?\",\n",
      "    \"options\": [\"A way to use multiple inputs for a single output\", \"A type of recurrent connection that allows information from previous time steps to affect the current time step\", \"A way to speed up computation by using less parameters\", \"A type of activation function used in LSTMs\"],\n",
      "    \"correct_answer\": 2\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is one technical detail of LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"They can only be used for simple problems\",\n",
      "      \"LSTM cells have a fixed dimension size\",\n",
      "      \"The relationship between c t -1 and the gate values is 1 to 1\",\n",
      "      \"LSTMs are always faster than GRUs\"\n",
      "    ],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is an advantage of using GRU over LSTM?\",\n",
      "    \"options\": [\n",
      "      \"GRUs are more powerful and flexible\",\n",
      "      \"GRUs are simpler models\",\n",
      "      \"LSTMs can capture longer range dependencies\",\n",
      "      \"GRUs are only used for simple problems\"\n",
      "    ],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"When should you use GRU over LSTM?\",\n",
      "    \"options\": [\n",
      "      \"When you need to build bigger models\",\n",
      "      \"When you want to capture longer range dependencies\",\n",
      "      \"GRUs are simpler and easier to scale\",\n",
      "      \"LSTMs have been historically proven\"\n",
      "    ],\n",
      "    \"correct_answer\": 3\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that the questions and options are based on the provided text, but some inferences were made to create coherent and logical questions.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a peephole connection in LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"A feature that allows the LSTM to learn from multiple time steps\",\n",
      "      \"A way of improving the memory efficiency of the model\",\n",
      "      \"A type of connection where the gate values depend on both a t-1 and x t\",\n",
      "      \"A simplification of the LSTM model\"\n",
      "    ],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which algorithm is generally considered more powerful and flexible?\",\n",
      "    \"options\": [\n",
      "      \"GRU\",\n",
      "      \"LSTM\",\n",
      "      \"Both are equally powerful\",\n",
      "      \"Neither is more powerful than the other\"\n",
      "    ],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why do some people prefer to use GRUs over LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"Because they are easier to build and train\",\n",
      "      \"Because they are simpler to understand and interpret\",\n",
      "      \"Because they can capture longer range dependencies\",\n",
      "      \"Because they are slower than LSTMs\"\n",
      "    ],\n",
      "    \"correct_answer\": 2\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that the correct answers are based on the provided text and may not be universally true.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a peephole connection in LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"A type of memory cell\",\n",
      "      \"A way to reduce computational complexity\",\n",
      "      \"A method where the gate values depend on both a t-1 and x t\",\n",
      "      \"A type of activation function\"\n",
      "    ],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which algorithm is considered more powerful and flexible?\",\n",
      "    \"options\": [\n",
      "      \"GRU\",\n",
      "      \"LSTM\",\n",
      "      \"Both are equally effective\",\n",
      "      \"It depends on the problem\"\n",
      "    ],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why do some researchers prefer to use GRUs over LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"Because they are simpler and easier to build\",\n",
      "      \"Because they are more computationally efficient\",\n",
      "      \"Because they have been gaining momentum in recent years\",\n",
      "      \"Because they are better suited for certain types of problems\"\n",
      "    ],\n",
      "    \"correct_answer\": 3\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note that I've reformatted the text to fit the JSON format, and also reworded some of the questions to make them more concise and clear. Let me know if you have any further requests!\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text in JSON format:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a key advantage of using GRUs compared to LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"LSTMs have more gates than GRUs.\",\n",
      "      \"GRUs are simpler and scale better to larger models.\",\n",
      "      \"LSTMs can capture longer range dependencies than GRUs.\",\n",
      "      \"GRUs are faster than LSTMs.\"\n",
      "    ],\n",
      "    \"correct_answer\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is a peephole connection in the context of LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"A type of activation function used in LSTMs.\",\n",
      "      \"A way to improve memory cell initialization.\",\n",
      "      \"A technique where the gate values depend on both the current input and the previous memory cell value.\",\n",
      "      \"A method to add more layers to an LSTM.\"\n",
      "    ],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is a widely accepted consensus among researchers about when to use GRUs versus LSTMs?\",\n",
      "    \"options\": [\n",
      "      \"Use GRUs for all problems and LSTMs only as a fallback.\",\n",
      "      \"Use GRUs for simple problems and LSTMs for complex problems.\",\n",
      "      \"Use LSTMs for simpler models and GRUs for more complex models.\",\n",
      "      \"There is no universally superior algorithm, so use both as needed.\"\n",
      "    ],\n",
      "    \"correct_answer\": 3\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note: The correct answers are based on the text provided and may not be universally accepted or proven in all contexts.\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the text in the JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is a key advantage of using a GRU over an LSTM?\",\n",
      "    \"options\": [\"GRUs are more powerful and flexible\", \"GRUs are simpler to build and scale\", \"GRUs are faster to compute with two gates instead of three\", \"GRUs are better for memorizing certain values\"],\n",
      "    \"correct_answer\": 2\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Why do LSTMs tend to be the historically more proven choice?\",\n",
      "    \"options\": [\"They are simpler to build and scale\", \"They can capture longer range dependencies with three gates\", \"They have been widely used for many years\", \"They are faster to compute\"],\n",
      "    \"correct_answer\": 3\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is the result of connecting multiple LSTMs in parallel?\",\n",
      "    \"options\": [\"The output is always C0 for all time steps\", \"It's impossible to capture long-range dependencies with multiple LSTMs\", \"The LSTM can easily remember certain values for a long time\", \"It's relatively easy to have the LSTM pass the value C0 all the way to the right\"],\n",
      "    \"correct_answer\": 4\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "generate_questions_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_descriptive_from_transcript(filename, model='llama3.2'):\n",
    "    task_description = \"\"\"\n",
    "    You are an AI tasked with generating descriptive questions from a given transcript. \n",
    "    Your goal is to:\n",
    "    1. Identify key concepts, events, or details in the transcript.\n",
    "    2. Frame questions that require a detailed and thoughtful written response, based on these concepts.\n",
    "    3. Ensure the questions are clear, specific, and prompt the reader to reflect or explain in-depth about the subject.\n",
    "\n",
    "    Example output:\n",
    "    [\n",
    "        \"Describe the main challenges faced during the event mentioned in the transcript.\",\n",
    "        \"What are the key factors contributing to the success of the project discussed in the transcript?\",\n",
    "        \"Explain the significance of the approach mentioned in addressing the problem outlined in the transcript.\"\n",
    "    ]\n",
    "\n",
    "    Your input will be a transcript, and you will generate 3 descriptive questions based on its content in the format of a JSON list:\n",
    "    [\n",
    "        \"<question1>\", \"<question2>\", \"<question3>\"\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        transcript = file.read()\n",
    "\n",
    "    prompt = task_description + '\\n Here is the transcript content: \\n' + transcript + 'Generate 3 questions as a JSON list, each question following the specified json format [\"<question1>\", \"<question2>\", \"<question3>\"].'\n",
    "\n",
    "    response = ollama.generate(model=model, prompt=prompt)\n",
    "\n",
    "    print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\"type\": \"multiple-choice\", \"question\": \"What is the main advantage of using self-attention mechanism?\", \"options\": [\"It allows to adapt the representation based on context\", \"It only uses one fixed word embedding for all words\", \"It ignores the relationships between words\"], \"correct\": \"1\"},\n",
      "  {\"type\": \"short-answer\", \"question\": \"How does the query, key, and value matrices contribute to the self-attention mechanism?\"},\n",
      "  {\"type\": \"fill-in-the-blank\", \"question\": \"The term 'scaled dot-product attention' is also known as <answer>\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON list of 3 questions:\n",
      "\n",
      "```\n",
      "[\n",
      "    \"What is the purpose of computing a query, key, and value for each word in the sequence?\",\n",
      "    \"How does the self-attention mechanism allow the representation to adapt to the context of each word?\",\n",
      "    \"What is the name of the attention mechanism represented in the original transformer architecture paper?\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\"type\": \"Multiple Choice\", \"question\": \"What is the primary purpose of the self-attention mechanism?\", \"options\": [\"to compute the sum of all word embeddings\", \"to select the most relevant words from the input sequence\", \"to adapt to the context and meaning of each word\"], \"answer\": \"3\"},\n",
      "    {\"type\": \"Short Answer\", \"question\": \"How does the key value in the self-attention mechanism help figure out which words provide the most relevant answer to a question about a given word?\"},\n",
      "    {\"type\": \"Open-Ended\", \"question\": \"What is the main advantage of using the self-attention mechanism over simply pulling up fixed word embeddings for each word in an input sequence?\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON list of 3 questions:\n",
      "\n",
      "```\n",
      "[\n",
      "    \"What is the purpose of using the query, key, and value in the self-attention mechanism?\",\n",
      "    \"How does the scaled dot-product attention relate to the original transformer architecture paper?\",\n",
      "    \"Can you explain how the self-attention mechanism allows for a richer representation of words based on their context?\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"q\": \"What is the primary purpose of the query matrix (Q) in the self-attention mechanism?\", \"type\": \"multiple-choice\", \"options\": [\"To ask a question about each word\", \"To look at all other words and determine relevance\", \"To represent each word as a value in the final representation\"]}, {\"q\": \"What is the role of the key matrix (K) in the self-attention mechanism?\", \"type\": \"multiple-choice\", \"options\": [\"To represent each word as a value in the final representation\", \"To ask a question about each word\", \"To look at all other words and determine relevance\"]}, {\"q\": \"What is the result of applying Softmax to the dot-product of Q, K, and V?\"\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "[\n",
      "  \"What is the purpose of the self-attention mechanism in the transformer network?\",\n",
      "  \"How does the query, key, and value matrices contribute to the computation of A^3?\",\n",
      "  \"Why is the representation of Africa more nuanced and richer compared to using a fixed word embedding?\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the generated JSON list of questions:\n",
      "\n",
      "```\n",
      "[\n",
      "  \"What is the purpose of computing a query, key, and value for each word in the sequence?\",\n",
      "  \"How does the self-attention mechanism allow the representation of a word to adapt based on its context?\",\n",
      "  \"What is the benefit of using multiple queries, keys, and values in parallel, as opposed to a single fixed word embedding?\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the text:\n",
      "\n",
      "```\n",
      "[\n",
      "  \"What is the main purpose of using a self-attention mechanism in the transformer network?\",\n",
      "  \"How does the query, key, and value vectors work together to represent each word in the sequence?\",\n",
      "  \"What is the difference between this type of attention and the scaled dot-product attention?\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_descriptive_from_transcript(\"subtitle.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "\n",
    "    # required but ignored\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Say this is a test',\n",
    "        }\n",
    "    ],\n",
    "    model='llama3.2',\n",
    ")\n",
    "\n",
    "completion = client.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    prompt=\"Say this is a test\",\n",
    ")\n",
    "\n",
    "list_completion = client.models.list()\n",
    "\n",
    "model = client.models.retrieve(\"llama3.2\")\n",
    "\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_transcript(filename, model='llama3.2'):\n",
    "    task_description = \"\"\"\n",
    "    You are an AI tasked with generating questions from a transcript. You must EXACTLY follow this format and structure.\n",
    "    \n",
    "    STRICT REQUIREMENTS:\n",
    "    1. Generate EXACTLY 2 questions for each of the 5 categories\n",
    "    2. Output must be valid JSON matching the example structure below\n",
    "    3. DO NOT include any explanatory text, commentary, or additional content\n",
    "    4. ONLY return the JSON object\n",
    "    5. Stick to the format specified\n",
    "\n",
    "    FORMAT:\n",
    "    {\n",
    "        \"multiple/single correct answers\": [\n",
    "            {\n",
    "                \"question\": \"<Your question here>\",\n",
    "                \"options\": [\"<Option1>\", \"<Option2>\", \"<Option3>\", \"<Option4>\"],\n",
    "                \"correct_answers\": [<index_of_correct_option1>, <index_of_correct_option2>,...]\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"True/False\": [\n",
    "            {\n",
    "                \"question\": \"<Your question here>\",\n",
    "                \"correct_answer\": <true_or_false>\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"Descriptive\": [\n",
    "            {\n",
    "                \"question\": \"<Your question here>\",\n",
    "                \"answer\": \"<Your answer here>\"\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"Numerical\": [\n",
    "            {\n",
    "                \"question\": \"<Your question here>\",\n",
    "                \"answer\": <Your answer here>\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    YOUR TASK:\n",
    "    1. Read the provided transcript carefully\n",
    "    2. Generate questions following EXACTLY the same structure as the example above\n",
    "    3. Ensure all questions are based on actual content from the transcript\n",
    "    4. Include EXACTLY 2 questions per category\n",
    "    5. Follow these specific rules:\n",
    "       - Single correct answer questions must have exactly 4 options and one correct answer\n",
    "       - Multiple correct answers must have 2-3 correct options from 4 total options\n",
    "       - True/False questions must be definitively true or false based on the transcript\n",
    "       - Descriptive answers must be 2-3 sentences long\n",
    "       - Numerical answers must be single numbers without units or text\n",
    "    6. Use 0-based indexing for all array indices\n",
    "    7. Return ONLY the JSON object with no additional text\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        transcript = file.read()\n",
    "\n",
    "    prompt = (\n",
    "        f\"{task_description}\\n\\n\"\n",
    "        f\"TRANSCRIPT:\\n{transcript}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Generate response\n",
    "        response = ollama.generate(model=model, prompt=prompt)\n",
    "\n",
    "        print(response[\"response\"])\n",
    "        \n",
    "        # Parse response to ensure valid JSON\n",
    "        import json\n",
    "        questions = json.loads(response[\"response\"])\n",
    "        \n",
    "        # Validate structure\n",
    "        required_categories = [\n",
    "            \"Single correct answer\",\n",
    "            \"multiple correct answers\",\n",
    "            \"True/False\",\n",
    "            \"Descriptive\",\n",
    "            \"Numerical\"\n",
    "        ]\n",
    "        \n",
    "        for category in required_categories:\n",
    "            if category not in questions:\n",
    "                raise ValueError(f\"Missing category: {category}\")\n",
    "            if len(questions[category]) != 2:\n",
    "                raise ValueError(f\"Category {category} must have exactly 2 questions\")\n",
    "        \n",
    "        return questions\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Generated response is not valid JSON\"}\n",
    "    except ValueError as e:\n",
    "        return {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Unexpected error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main concept of the self-attention mechanism in the transformer network is to use a query, key, and value matrix to represent each word in a sequence. The query matrix represents the question or context being asked about a particular word, such as \"what's happening in Africa\". The key matrix represents all the other words in the sequence, and by comparing their similarity to the query matrix, you can determine which words provide the most relevant answers.\n",
      "\n",
      "The value matrix allows the representation of each word to be plugged into the final output, so that the representation of a word takes into account its relationships with other words in the sequence. This results in a richer and more nuanced representation for each word than if it were represented by a fixed word embedding alone.\n",
      "\n",
      "The key advantage of this mechanism is that it allows the model to adapt to the context of each word, taking into account not just its own meaning but also its relationships with surrounding words. This enables the model to capture complex semantic relationships between words in a sequence, which is important for tasks such as natural language understanding and generation.\n",
      "\n",
      "The self-attention mechanism can be applied multiple times to different parts of the input sequence, resulting in a multi-headed attention mechanism that allows the model to weigh the importance of different word sequences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'Generated response is not valid JSON'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_questions_from_transcript('subtitle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
